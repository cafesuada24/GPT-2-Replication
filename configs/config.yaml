model:
  tokenizer:
    name: "tiktoken"
    init_args:
      encoding_name: "gpt2" 
  config:
    vocab_size: 50257
    context_length: 1024
    drop_rate: 0.1
    qkv_bias: False
  model_size: "124M"

model_configs:
  124M:
    d_emb: 768
    n_layers: 12
    n_heads: 12
  355M:
    d_emb: 1024
    n_layers: 24
    n_heads: 16
  774M:
    d_emb: 1280
    n_layers: 36
    n_heads: 20
  1558M:
    d_emb: 1600
    n_layers: 48
    n_heads: 25

paths:
  data_dir: "data/"
  log_dir: "logs/"
  model_dir: "checkpoints/"
